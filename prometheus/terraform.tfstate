{
  "version": 4,
  "terraform_version": "1.14.0",
  "serial": 169,
  "lineage": "14ab97b6-16d6-6a5e-d216-d85226b0fc0b",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "kube_prom_stack",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "kube-prom-stack",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.86.2",
                "chart": "kube-prometheus-stack",
                "first_deployed": 1764507559,
                "last_deployed": 1764507559,
                "name": "kube-prom-stack",
                "namespace": "monitoring",
                "notes": "kube-state-metrics is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.\nThe exposed metrics can be found here:\nhttps://github.com/kubernetes/kube-state-metrics/blob/master/docs/README.md#exposed-metrics\n\nThe metrics are exported on the HTTP endpoint /metrics on the listening port.\nIn your case, kube-prom-stack-kube-state-metrics.monitoring.svc.cluster.local:8080/metrics\n\nThey are served either as plaintext or protobuf depending on the Accept header.\nThey are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint.\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace monitoring -l \"app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/instance=kube-prom-stack\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9100 to use your application\"\n  kubectl port-forward --namespace monitoring $POD_NAME 9100\n1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace monitoring kube-prom-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   kube-prom-stack-grafana.monitoring.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\n     export NODE_PORT=$(kubectl get --namespace monitoring -o jsonpath=\"{.spec.ports[0].nodePort}\" services kube-prom-stack-grafana)\n     export NODE_IP=$(kubectl get nodes --namespace monitoring -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n     echo http://$NODE_IP:$NODE_PORT\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace monitoring get pods -l \"release=kube-prom-stack\"\n\nGet Grafana 'admin' user password by running:\n\n  kubectl --namespace monitoring get secrets kube-prom-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 -d ; echo\n\nAccess Grafana local instance:\n\n  export POD_NAME=$(kubectl --namespace monitoring get pod -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=kube-prom-stack\" -oname)\n  kubectl --namespace monitoring port-forward $POD_NAME 3000\n\nGet your grafana admin user password by running:\n\n  kubectl get secret --namespace monitoring -l app.kubernetes.io/component=admin-secret -o jsonpath=\"{.items[0].data.admin-password}\" | base64 --decode ; echo\n\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026 configure Alertmanager and Prometheus instances using the Operator.\n",
                "revision": 1,
                "values": "{\"alertmanager\":{\"alertmanagerSpec\":{\"configSecret\":\"alertmanager-slack\"},\"enabled\":true},\"grafana\":{\"adminPassword\":\"admin\",\"adminUser\":\"admin\",\"enabled\":true,\"service\":{\"nodePort\":30080,\"type\":\"NodePort\"}},\"kubeStateMetrics\":{\"enabled\":true},\"nodeExporter\":{\"enabled\":true},\"prometheus\":{\"prometheusSpec\":{\"podMonitorSelector\":{},\"serviceMonitorSelector\":{}}}}",
                "version": "79.9.0"
              }
            ],
            "name": "kube-prom-stack",
            "namespace": "monitoring",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "upgrade_install": null,
            "values": [
              "# Prometheus settings\nprometheus:\n  prometheusSpec:\n    serviceMonitorSelector: {}\n    podMonitorSelector: {}\n\n# Grafana settings\ngrafana:\n  enabled: true\n  adminUser: admin\n  adminPassword: admin\n  service:\n    type: NodePort\n    nodePort: 30080\n\n# Alertmanager settings\nalertmanager:\n  enabled: true\n  alertmanagerSpec:\n    configSecret: alertmanager-slack\n\n\n# Node Exporter \u0026 Kube State Metrics\nkubeStateMetrics:\n  enabled: true\n\nnodeExporter:\n  enabled: true\n"
            ],
            "verify": false,
            "version": "79.9.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "identity_schema_version": 0,
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubernetes_namespace.monitoring",
            "null_resource.alertmanager_slack"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "monitoring",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "monitoring",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "monitoring": "prometheus"
                },
                "name": "monitoring",
                "resource_version": "71906",
                "uid": "3c080dbd-a43c-412a-bb49-1213f7e38623"
              }
            ],
            "timeouts": null,
            "wait_for_default_service_account": false
          },
          "sensitive_attributes": [],
          "identity_schema_version": 1,
          "identity": {
            "api_version": "v1",
            "kind": "Namespace",
            "name": "monitoring"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "null_resource",
      "name": "alertmanager_slack",
      "provider": "provider[\"registry.terraform.io/hashicorp/null\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "817341436341893646",
            "triggers": null
          },
          "sensitive_attributes": [],
          "identity_schema_version": 0,
          "dependencies": [
            "kubernetes_namespace.monitoring"
          ]
        }
      ]
    }
  ],
  "check_results": null
}
